{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obsidian News Digest -prototype\n",
    "\n",
    "In this notebook, we'll build an automated news digest tool step by step. You'll implement each component with guidance, run the code to see results, and gradually build a complete working system.\n",
    "\n",
    "Our tool will:\n",
    "- Fetch news articles from popular sources\n",
    "- Summarize articles using LangChain and a language model\n",
    "- Format summaries in Markdown\n",
    "- Save the digest to an Obsidian vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "üìÅ Output path: D:\\Obsidian_VauLTs\\My_Daily_newS__\\Daily_news\n",
      "üì∞ News sources: 2 sources configured\n",
      "üìä Max articles: 10\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: CONFIGURATIONS\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if required environment variables are set\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "vault_path = os.getenv(\"OBSIDIAN_VAULT_PATH\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è OPENAI_API_KEY not found! Make sure to set it in your .env file.\")\n",
    "    \n",
    "    \n",
    "if not vault_path:\n",
    "    print(\"‚ö†Ô∏è OBSIDIAN_VAULT_PATH not found! Make sure to set it in your .env file.\")\n",
    "    \n",
    "    vault_path = \"./output\"  # Default output folder\n",
    "    \n",
    "# Configuration variables\n",
    "news_sources = [\n",
    "    \"https://www.apnews.com/\",\n",
    "    \"https://www.c-span.org/\"\n",
    "]\n",
    "max_articles = 10  # Maximum number of articles in the digest\n",
    "output_folder = \"Daily_news\"  # Folder within Obsidian vault\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"üìÅ Output path: {os.path.join(vault_path, output_folder)}\")\n",
    "print(f\"üì∞ News sources: {len(news_sources)} sources configured\")\n",
    "print(f\"üìä Max articles: {max_articles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: FETCHING NEWS\n",
    "\n",
    "# Import newspaper3k library\n",
    "from newspaper import Article, build\n",
    "\n",
    "def fetch_news(source_urls: List[str], max_articles_per_source: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch news articles from multiple sources using newspaper3k.\n",
    "    \n",
    "    Args:\n",
    "        source_urls: List of news source URLs to fetch from\n",
    "        max_articles_per_source: Maximum number of articles to fetch per source\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing article information\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    \n",
    "    for url in source_urls:\n",
    "        try:\n",
    "            print(f\"Fetching from {url}...\")\n",
    "            \n",
    "            # Build newspaper from source URL - this analyzes the site to find articles\n",
    "            paper = build(url)\n",
    "            \n",
    "            # Get all article URLs from the source\n",
    "            article_urls = paper.article_urls()\n",
    "            print(f\"Found {len(article_urls)} article links\")\n",
    "            \n",
    "            # Get the most recent/prominent articles \n",
    "            article_urls_list = list(article_urls)\n",
    "            # Most news sites list headlines/important articles first in their HTML\n",
    "            sampled_urls = article_urls_list[:min(max_articles_per_source * 2, len(article_urls_list))]     \n",
    "            \n",
    "            source_articles = []\n",
    "            for article_url in sampled_urls:\n",
    "                try:\n",
    "                    # Create an article object\n",
    "                    article = Article(article_url)\n",
    "                    \n",
    "                    # Download the article content\n",
    "                    article.download()\n",
    "                    time.sleep(1)  # Pause briefly to be polite to the server\n",
    "                    \n",
    "                    # Parse the article to extract content\n",
    "                    article.parse()\n",
    "                    \n",
    "                    # Skip articles with minimal content (likely not full articles)\n",
    "                    if not article.text or len(article.text) < 100:\n",
    "                        continue\n",
    "                        \n",
    "                    # Add article information to our collection\n",
    "                    source_articles.append({\n",
    "                        \"title\": article.title,\n",
    "                        \"url\": article.url,\n",
    "                        \"text\": article.text[:2000],  # Limit text length for LLM (saves tokens)\n",
    "                        \"published_date\": article.publish_date,\n",
    "                        \"source\": url\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"‚úì Downloaded: {article.title[:50]}...\")\n",
    "                    \n",
    "                    # Stop once we have enough articles from this source\n",
    "                    if len(source_articles) >= max_articles_per_source:\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article {article_url}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Add articles from this source to our master list\n",
    "            all_articles.extend(source_articles)\n",
    "            print(f\"Got {len(source_articles)} articles from {url}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing source {url}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing news fetcher...\n",
      "Fetching from https://www.naftemporiki.gr/...\n",
      "Found 2 article links\n",
      "‚úì Downloaded: Œó ŒïŒªŒªŒ¨Œ¥Œ± Œ±ŒΩŒ±ŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπ œÑŒ∑ŒΩ Œ†œÅŒøŒµŒ¥œÅŒØŒ± œÑŒøœÖ Œ£œÖŒºŒ≤ŒøœÖŒªŒØŒøœÖ Œë...\n",
      "Got 1 articles from https://www.naftemporiki.gr/\n",
      "\n",
      "Retrieved 1 articles\n",
      "\n",
      "SAMPLE ARTICLE:\n",
      "Title: Œó ŒïŒªŒªŒ¨Œ¥Œ± Œ±ŒΩŒ±ŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπ œÑŒ∑ŒΩ Œ†œÅŒøŒµŒ¥œÅŒØŒ± œÑŒøœÖ Œ£œÖŒºŒ≤ŒøœÖŒªŒØŒøœÖ ŒëœÉœÜŒ±ŒªŒµŒØŒ±œÇ œÑŒøœÖ ŒüŒóŒï\n",
      "URL: https://www.naftemporiki.gr/politics/1950756/i-ellada-analamvanei-tin-proedria-toy-symvoylioy-asfaleias-toy-oie/\n",
      "Text length: 692 characters\n",
      "Text preview: Œ§Œ∑ŒΩ Œ†œÅŒøŒµŒ¥œÅŒØŒ± œÑŒøœÖ Œ£œÖŒºŒ≤ŒøœÖŒªŒØŒøœÖ ŒëœÉœÜŒ±ŒªŒµŒØŒ±œÇ œÑŒøœÖ ŒüœÅŒ≥Œ±ŒΩŒπœÉŒºŒøœç ŒóŒΩœâŒºŒ≠ŒΩœâŒΩ ŒïŒ∏ŒΩœéŒΩ (ŒüŒóŒï) Œ±ŒΩŒ±ŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπ Œ±œÄœå Œ±œçœÅŒπŒø, 1Œ∑ ŒúŒ±ŒêŒøœÖ 2025, Œ∑ ŒïŒªŒªŒ¨Œ¥Œ± Œ∫Œ±Œπ Œ±ŒΩŒ±ŒºŒ≠ŒΩŒµœÑŒ±Œπ ŒΩŒ± Œ¥ŒπŒ±œÅŒ∫Œ≠œÉŒµŒπ Œ≠ŒΩŒ± ŒºŒÆŒΩŒ±.\n",
      "\n",
      "Œ£œçŒºœÜœâŒΩŒ± ŒºŒµ œÄŒªŒ∑œÅŒøœÜŒøœÅŒØŒµœÇ œÑŒ∑œÇ ¬´Œù¬ª, œÉœÑŒπœÇ 20 ...\n"
     ]
    }
   ],
   "source": [
    "# Test the news fetcher with a single source and limited articles\n",
    "test_source = [news_sources[0]]  # Just use the first source (BBC)\n",
    "print(\"üîç Testing news fetcher...\")\n",
    "test_articles = fetch_news(test_source, max_articles_per_source=1)\n",
    "\n",
    "# Print information about the retrieved articles\n",
    "print(f\"\\nRetrieved {len(test_articles)} articles\")\n",
    "if test_articles:\n",
    "    article = test_articles[0]\n",
    "    print(f\"\\nSAMPLE ARTICLE:\")\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"URL: {article['url']}\")\n",
    "    print(f\"Text length: {len(article['text'])} characters\")\n",
    "    print(f\"Text preview: {article['text'][:200]}...\")\n",
    "else:\n",
    "    print(\"No articles found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: SUMMARIZING NEWS\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def summarize_and_format_articles(articles: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Summarize news articles and format them as a complete markdown digest.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of article dictionaries with title, text, etc.\n",
    "        \n",
    "    Returns:\n",
    "        Formatted markdown string of the complete digest\n",
    "    \"\"\"\n",
    "    # Initialize the OpenAI chat model\n",
    "    llm = ChatOpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=\"gpt-4.1-nano-2025-04-14\", \n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    # Get today's date for the digest header\n",
    "    today = datetime.now().strftime(\"%d %b %Y\")\n",
    "    \n",
    "    # Handle case with no articles\n",
    "    if not articles:\n",
    "        return f\"No major news today.\"\n",
    "    \n",
    "    # Process each article\n",
    "    article_summaries = []\n",
    "    for i, article in enumerate(articles):\n",
    "        try:\n",
    "            print(f\"Summarizing article {i+1}/{len(articles)}...\")\n",
    "            \n",
    "            # Extract source domain for display\n",
    "            source_url = article['source']\n",
    "            source_domain = source_url.split('//')[1].split('/')[0].replace('www.', '')\n",
    "            \n",
    "            # Create prompt template for article summarization with formatting\n",
    "            prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "                You are a Veteran News Journalist.\n",
    "                Summarize this news article in 5 sentences. \n",
    "                Focus on the main facts and key details.\n",
    "                \n",
    "                Title: {title}\n",
    "                \n",
    "                Article: {text}\n",
    "                \n",
    "                Format your response in this exact format:\n",
    "                \n",
    "                ## {title}\n",
    "                \n",
    "                [Your 5 sentence summary here]\n",
    "                \n",
    "                *Source: {source_domain}*\n",
    "                \n",
    "                [Read more ‚Üó]({url})\n",
    "                \n",
    "                ---\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            # Create and invoke the chain (prompt -> LLM)\n",
    "            chain = prompt | llm\n",
    "            response = chain.invoke({\n",
    "                \"title\": article[\"title\"], \n",
    "                \"text\": article[\"text\"],\n",
    "                \"source_domain\": source_domain,\n",
    "                \"url\": article[\"url\"]\n",
    "            })\n",
    "            \n",
    "            # Add the formatted summary to our list\n",
    "            article_summaries.append(response.content)\n",
    "            print(f\"‚úì Summarized: {article['title'][:50]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing article '{article['title']}': {e}\")\n",
    "            # Add a placeholder for failed articles\n",
    "            article_summaries.append(f\"## {article['title']}\\n\\nSummary unavailable.\\n\\n---\\n\")\n",
    "    \n",
    "    # Combine everything - note that we're NOT adding the title header\n",
    "    # The filename will serve as the title in Obsidian\n",
    "    digest = \"\\n\".join(article_summaries)\n",
    "    \n",
    "    return digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing summarization...\n",
      "Summarizing article 1/1...\n",
      "‚úì Summarized: Œó ŒïŒªŒªŒ¨Œ¥Œ± Œ±ŒΩŒ±ŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπ œÑŒ∑ŒΩ Œ†œÅŒøŒµŒ¥œÅŒØŒ± œÑŒøœÖ Œ£œÖŒºŒ≤ŒøœÖŒªŒØŒøœÖ Œë...\n",
      "\n",
      "Generated digest:\n",
      "-------------------\n",
      "## Œó ŒïŒªŒªŒ¨Œ¥Œ± Œ±ŒΩŒ±ŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπ œÑŒ∑ŒΩ Œ†œÅŒøŒµŒ¥œÅŒØŒ± œÑŒøœÖ Œ£œÖŒºŒ≤ŒøœÖŒªŒØŒøœÖ ŒëœÉœÜŒ±ŒªŒµŒØŒ±œÇ œÑŒøœÖ ŒüŒóŒï\n",
      "\n",
      "ŒëœÄœå Œ±œçœÅŒπŒø, 1Œ∑ ŒúŒ±ŒêŒøœÖ 2025, Œ∑ ŒïŒªŒªŒ¨Œ¥Œ± Œ∏Œ± Œ±ŒΩŒ±ŒªŒ¨Œ≤ŒµŒπ œÑŒ∑ŒΩ Œ†œÅŒøŒµŒ¥œÅŒØŒ± œÑŒøœÖ Œ£œÖŒºŒ≤ŒøœÖŒªŒØŒøœÖ ŒëœÉœÜŒ±ŒªŒµŒØŒ±œÇ œÑŒøœÖ ŒüŒóŒï, ŒºŒµ Œ±ŒΩŒ±ŒºŒµŒΩœåŒºŒµŒΩŒ∑ Œ¥ŒπŒ¨œÅŒ∫ŒµŒπŒ± Œ≠ŒΩŒ± ŒºŒÆŒΩŒ±. Œ£œÑŒπœÇ 20 ŒúŒ±ŒêŒøœÖ 2025, Œ∏Œ± œÄœÅŒ±Œ≥ŒºŒ±œÑŒøœÄŒøŒπŒ∑Œ∏ŒµŒØ œÉœÑŒπœÇ ŒóŒΩœâŒºŒ≠ŒΩŒµœÇ Œ†ŒøŒªŒπœÑŒµŒØŒµœÇ ŒëŒºŒµœÅŒπŒ∫ŒÆœÇ Œ∫ŒµŒΩœÑœÅŒπŒ∫ŒÆ ŒµŒ∫Œ¥ŒÆŒªœâœÉŒ∑ œÑŒ∑œÇ ŒïŒªŒªŒ∑ŒΩŒπŒ∫ŒÆœÇ Œ†œÅŒøŒµŒ¥œÅŒØŒ±œÇ, ŒºŒµ œÑŒ∑ŒΩ œÄŒ±œÅŒøœÖœÉŒØŒ± œÑŒøœÖ Œ†œÅœâŒ∏œÖœÄŒøœÖœÅŒ≥Œøœç ŒöœÖœÅŒπŒ¨Œ∫ŒøœÖ ŒúŒ∑œÑœÉŒøœÑŒ¨Œ∫Œ∑. Œó Œ∫ŒµŒΩœÑœÅŒπŒ∫ŒÆ ŒµŒ∫Œ¥ŒÆŒªœâœÉŒ∑ Œ∏Œ± ŒµŒØŒΩŒ±Œπ Œ∑ Œ£œÖŒΩŒµŒ¥œÅŒØŒ±œÉŒ∑ œÑŒøœÖ ŒüŒóŒï ŒºŒµ Œ∏Œ≠ŒºŒ± œÑŒ∑ŒΩ Œ∏Œ±ŒªŒ¨œÉœÉŒπŒ± Œ±œÉœÜŒ¨ŒªŒµŒπŒ±. Œü Œ†œÅœâŒ∏œÖœÄŒøœÖœÅŒ≥œåœÇ Œ∏Œ± Œ±œÄŒµœÖŒ∏œçŒΩŒµŒπ ŒøŒºŒπŒªŒØŒ± Œ∫Œ±Œπ Œ∏Œ± œÄœÅŒøŒµŒ¥œÅŒµœçœÉŒµŒπ œÑŒ∑œÇ Œ£œÖŒΩŒµŒ¥œÅŒØŒ±œÉŒ∑œÇ œÑŒøœÖ Œ£œÖŒºŒ≤ŒøœÖŒªŒØŒøœÖ ŒëœÉœÜŒ±ŒªŒµŒØŒ±œÇ œÑŒøœÖ ŒüŒóŒï œÉœÑŒ∑ ŒùŒ≠Œ± Œ•œåœÅŒ∫Œ∑.\n"
     ]
    }
   ],
   "source": [
    "# Test with one of the articles we just fetched\n",
    "if test_articles and api_key:\n",
    "    print(\"Testing summarization...\")\n",
    "    digest = summarize_and_format_articles([test_articles[0]])\n",
    "    print(\"\\nGenerated digest:\")\n",
    "    print(\"-------------------\")\n",
    "    print(digest)\n",
    "else:\n",
    "    print(\"Cannot test summarization: No articles or API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: PUBLISHING TO OBSIDIAN\n",
    "\n",
    "def publish_to_obsidian(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Publish the formatted digest to Obsidian vault.\n",
    "    \n",
    "    Args:\n",
    "        content: Formatted markdown content\n",
    "        \n",
    "    Returns:\n",
    "        Path to the created file\n",
    "    \"\"\"\n",
    "    # Get today's date for the filename\n",
    "    today = datetime.now().strftime(\"%d %b %Y\")\n",
    "    \n",
    "    # Create path to the output folder in the vault\n",
    "    folder_path = os.path.join(vault_path, output_folder)\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Create file path with a clear descriptive name\n",
    "    file_name = f\"Global News Digest ‚Äì {today}.md\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Write content to file\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application\n",
    "\n",
    "def create_news_digest(sources=None, max_articles_count=None):\n",
    "    \"\"\"\n",
    "    Execute the complete news digest workflow\n",
    "    \"\"\"\n",
    "    # Use provided parameters or defaults\n",
    "    if sources is None:\n",
    "        sources = news_sources\n",
    "        \n",
    "    if max_articles_count is None:\n",
    "        max_articles_count = max_articles\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Fetch news articles\n",
    "        print(f\"üì∞ Fetching news from {len(sources)} sources...\")\n",
    "        articles = fetch_news(sources, max_articles_per_source=max_articles_count//len(sources))\n",
    "        print(f\"Retrieved {len(articles)} articles.\")\n",
    "        \n",
    "        # Take top articles if we have more than max_articles_count\n",
    "        selected_articles = articles[:max_articles_count]\n",
    "        print(f\"Selected {len(selected_articles)} articles for summarization.\")\n",
    "        \n",
    "        # Step 2: Summarize and format articles\n",
    "        print(\"üîç Summarizing and formatting articles...\")\n",
    "        digest = summarize_and_format_articles(selected_articles)\n",
    "        \n",
    "        # Step 3: Publish to Obsidian\n",
    "        print(\"üíæ Publishing to Obsidian...\")\n",
    "        file_path = publish_to_obsidian(digest)\n",
    "        \n",
    "        print(f\"‚úÖ News digest published successfully to: {file_path}\")\n",
    "        return file_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in news digest pipeline: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Fetching news from 1 sources...\n",
      "Fetching from https://www.naftemporiki.gr/...\n",
      "Found 0 article links\n",
      "Got 0 articles from https://www.naftemporiki.gr/\n",
      "Retrieved 0 articles.\n",
      "Selected 0 articles for summarization.\n",
      "üîç Summarizing and formatting articles...\n",
      "üíæ Publishing to Obsidian...\n",
      "‚úÖ News digest published successfully to: D:\\Obsidian_VauLTs\\My_Daily_newS__\\Daily_news\\Global News Digest ‚Äì 30 Apr 2025.md\n",
      "\n",
      "üéâ Success! Your news digest is ready at: D:\\Obsidian_VauLTs\\My_Daily_newS__\\Daily_news\\Global News Digest ‚Äì 30 Apr 2025.md\n",
      "Check your Obsidian vault to see the complete digest.\n"
     ]
    }
   ],
   "source": [
    "# Run the workflow with just one source and a small number of articles\n",
    "test_workflow_result = create_news_digest(\n",
    "    sources=[news_sources[0]],  # Just use BBC for testing\n",
    "    max_articles_count=2  # Limit to 2 articles for quick testing\n",
    ")\n",
    "\n",
    "if test_workflow_result:\n",
    "    print(f\"\\nüéâ Success! Your news digest is ready at: {test_workflow_result}\")\n",
    "    print(\"Check your Obsidian vault to see the complete digest.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Workflow failed. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Fetching news from 2 sources...\n",
      "Fetching from https://www.apnews.com/...\n",
      "Found 148 article links\n",
      "‚úì Downloaded: Kremlin says a deal to end the war with Ukraine ca...\n",
      "‚úì Downloaded: Vietnam celebrates 50 years since war‚Äôs end with f...\n",
      "‚úì Downloaded: Middle East latest: At least 12 killed overnight b...\n",
      "‚úì Downloaded: Immigrants working legally in the Texas Panhandle ...\n",
      "‚úì Downloaded: Takeaways from AP‚Äôs report on how Trump‚Äôs immigrat...\n",
      "Got 5 articles from https://www.apnews.com/\n",
      "Fetching from https://www.c-span.org/...\n",
      "Found 0 article links\n",
      "Got 0 articles from https://www.c-span.org/\n",
      "Retrieved 5 articles.\n",
      "Selected 5 articles for summarization.\n",
      "üîç Summarizing and formatting articles...\n",
      "Summarizing article 1/5...\n",
      "‚úì Summarized: Kremlin says a deal to end the war with Ukraine ca...\n",
      "Summarizing article 2/5...\n",
      "‚úì Summarized: Vietnam celebrates 50 years since war‚Äôs end with f...\n",
      "Summarizing article 3/5...\n",
      "‚úì Summarized: Middle East latest: At least 12 killed overnight b...\n",
      "Summarizing article 4/5...\n",
      "‚úì Summarized: Immigrants working legally in the Texas Panhandle ...\n",
      "Summarizing article 5/5...\n",
      "‚úì Summarized: Takeaways from AP‚Äôs report on how Trump‚Äôs immigrat...\n",
      "üíæ Publishing to Obsidian...\n",
      "‚úÖ News digest published successfully to: D:\\Obsidian_VauLTs\\My_Daily_newS__\\Daily_news\\Global News Digest ‚Äì 30 Apr 2025.md\n",
      "\n",
      "üéâ Success! Your complete news digest is ready at: D:\\Obsidian_VauLTs\\My_Daily_newS__\\Daily_news\\Global News Digest ‚Äì 30 Apr 2025.md\n",
      "Check your Obsidian vault to see the full digest.\n"
     ]
    }
   ],
   "source": [
    "# Run the complete workflow with all configured sources and max articles\n",
    "full_workflow_result = create_news_digest()\n",
    "\n",
    "if full_workflow_result:\n",
    "    print(f\"\\nüéâ Success! Your complete news digest is ready at: {full_workflow_result}\")\n",
    "    print(\"Check your Obsidian vault to see the full digest.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Complete workflow failed. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
